//!
//! \file skl_spsc_bidirectional_ring
//!
//! \brief Wait-free single producer single consumer bidirectional ring buffer
//!
//! \details Provides a lock-free SPSC queue where the producer enqueues objects,
//!          the consumer processes them, and processed results flow back to the
//!          producer. Objects flow bidirectionally (producer <-> consumer).
//!
//! \note Supports optional huge pages allocation for improved performance
//! \note When using huge pages, call allocate_internal_storage() before use
//!
//! \license Licensed under the MIT License. See LICENSE for details.
//!
#pragma once

#include "skl_int"
#include "skl_def"
#include "skl_atomic"
#include "skl_utility"
#include "skl_huge_pages"
#include "skl_traits/conditional_t"

namespace skl {
//! [Internal] Zero memory
void skl_core_zero_memory(void* f_block, u64 f_bytes_count) noexcept;
//! Concept to detect the [reset() noexcept] method on _Object
template <typename _Object>
concept spsc_bidirectional_ring_reset_method_c = requires(_Object f_object) {
    { f_object.reset() } noexcept;
};

//! [WaitFree] Single Consumer Single Producer, bidirectional ring buffer (queue)
template <typename _Object, u64 _Size, bool _UseHugePages>
    requires(__is_nothrow_constructible(_Object))
struct SKL_CACHE_ALIGNED spsc_bidirectional_ring_t {
    static constexpr u64 Size = _Size;
    static constexpr u64 Mask = _Size - 1U;
    static_assert((Size > 0ULL) && (0U == (Size & Mask)), "_Size must be a power of 2!");

    //! Whether to use huge pages for the internal buffer allocation
    static constexpr bool CUseHugePages   = _UseHugePages;
    static constexpr u64  CHugePagesCount = integral_ceil(sizeof(_Object) * Size, huge_pages::CHugePageSize);

    //! Storage type for the internal buffer
    using storage_t = conditional_t<_UseHugePages, _Object*, _Object[Size]>;

    SKL_NO_MOVE_OR_COPY(spsc_bidirectional_ring_t);

    spsc_bidirectional_ring_t() noexcept = default;
    ~spsc_bidirectional_ring_t() noexcept {
        if constexpr (_UseHugePages) {
            if (nullptr != m_objects) {
                free_internal_storage();
            }
        }
    }

    //! Allocate the internal storage
    //! \remark Must be called before any other operation if _UseHugePages is true
    //! \remark Must not be called more than once (asserts if already allocated)
    void allocate_internal_storage() noexcept
        requires(_UseHugePages)
    {
        SKL_ASSERT_PERMANENT(nullptr == m_objects && "Double allocation: internal storage already allocated");
        m_objects = reinterpret_cast<_Object*>(huge_pages::skl_huge_page_alloc(CHugePagesCount));
        SKL_ASSERT_PERMANENT(nullptr != m_objects);

        // Placement new all objects
        if constexpr (__is_trivially_constructible(_Object)) {
            skl_core_zero_memory(m_objects, sizeof(_Object) * Size);
        } else {
            for (u64 i = 0U; i < Size; ++i) {
                new (&m_objects[i]) _Object();
            }
        }
    }

    //! Free the internal storage
    //! \remark Must be called to free resources if _UseHugePages is true
    //! \remark Automatically called in the destructor
    void free_internal_storage() noexcept
        requires(_UseHugePages)
    {
        SKL_ASSERT_PERMANENT(nullptr != m_objects);

        // Destruct all objects
        if constexpr (false == __is_trivially_destructible(_Object)) {
            for (u64 i = 0U; i < Size; ++i) {
                m_objects[i].~_Object();
            }
        }

        huge_pages::skl_huge_page_free(m_objects, CHugePagesCount);
        m_objects = nullptr;
    }

    //! Does the internal storage exist
    [[nodiscard]] bool has_internal_storage() const noexcept
        requires(_UseHugePages)
    {
        return nullptr != m_objects;
    }

    //! Get the internal buffer
    //! \remark only use this method when the producer and consumer are not running
    //! \remark eg. use it to prepare the objects in the buffer in a specific way before use
    [[nodiscard]] _Object* buffer() noexcept {
        assert_storage_valid();
        return m_objects;
    }

    //! [SCSP] {Producer} Allocate new object
    //! \remark call submit() to submit all allocated objects to be visible to the consumer
    //! \returns nullptr if no object available for allocation
    [[nodiscard]] _Object* allocate() noexcept {
        assert_storage_valid();
        if (0U == free_count()) {
            return nullptr;
        }

        const auto alloc_index = m_allocate_head++;
        return &m_objects[(alloc_index & Mask)];
    }

    //! [SCSP] {Producer} Allocate new object
    //! \remark call submit() to submit all allocated objects to be visible to the consumer
    //! \remark asserts free_count() > 0
    [[nodiscard]] _Object& allocate_checked() noexcept {
        assert_storage_valid();
        SKL_ASSERT(0U < free_count());
        const auto alloc_index = m_allocate_head++;
        return m_objects[(alloc_index & Mask)];
    }

    //! [SCSP] {Producer} Allocate objects in bulk
    //! \remark call submit() to make all allocated objects visible to the consumer in a single (release) atomic operation
    //! \returns false the queue is full, no \p f_count free objects for allocation
    [[nodiscard]] bool allocate_bulk(_Object** f_out_objects, u32 f_count) noexcept {
        assert_storage_valid();
        if (f_count > free_count()) {
            return false;
        }

        const auto start = m_allocate_head;
        for (u64 i = 0ULL; i < f_count; ++i) {
            f_out_objects[i] = &m_objects[(start + i) & Mask];
        }
        m_allocate_head += f_count;

        return true;
    }

    //! [SCSP] {Producer} Allocate objects in bulk
    //! \remark call submit() to make all allocated objects visible to the consumer in a single (release) atomic operation
    //! \remark asserts free_count() >= \p f_count
    void allocate_bulk_checked(_Object** f_out_objects, u32 f_count) noexcept {
        assert_storage_valid();
        SKL_ASSERT(f_count <= free_count());

        const auto start = m_allocate_head;
        for (u64 i = 0ULL; i < f_count; ++i) {
            f_out_objects[i] = &m_objects[(start + i) & Mask];
        }
        m_allocate_head += f_count;
    }

    //! [SCSP] {Producer} Get count of free objects
    [[nodiscard]] u64 free_count() const noexcept {
        const u64 delta = m_allocate_head - m_end_tail;
        SKL_ASSERT_CRITICAL(delta <= Size);
        return Size - delta;
    }

    //! [SCSP] {Producer} Get allocated objects count
    [[nodiscard]] u64 allocated_count() const noexcept {
        return m_allocate_head - m_queue_head.load_relaxed();
    }

    //! [SCSP] {Producer} Submit all allocated objects (if any)
    void submit() noexcept {
        const auto head = m_queue_head.load_relaxed();
        SKL_ASSERT_CRITICAL(m_allocate_head >= head);
        if (m_allocate_head > head) {
            m_queue_head.store_release(m_allocate_head);
        }
    }

    //! [SCSP] {Consumer} Submit all processed objects
    void submit_results() noexcept {
        if (m_process_head > m_queue_tail.load_relaxed()) {
            m_queue_tail.store_release(m_process_head);
        }
    }

    //! [SCSP] {Consumer} Submit all processed objects
    //! \returns the count of submitted objects
    u32 submit_results_ex() noexcept {
        const auto delta = m_process_head - m_queue_tail.load_relaxed();
        if (0U < delta) {
            m_queue_tail.store_release(m_process_head);
        }
        return delta;
    }

    //! [SCSP] {Producer} Pop allocated object
    //! \remark asserts (0U < allocated_count())
    template <bool _ResetObjectIfPossible = true>
    void pop_allocation() noexcept {
        SKL_ASSERT_CRITICAL(0U < allocated_count());
        if constexpr (_ResetObjectIfPossible && spsc_bidirectional_ring_reset_method_c<_Object>) {
            m_objects[(m_allocate_head - 1U) & Mask].reset();
        }

        --m_allocate_head;
    }

    //! [SCSP] {Consumer} Collect objects that need processing (up to \p f_max_count)
    [[nodiscard]] u32 dequeue_burst(_Object** f_out_objects, u32 f_max_count) noexcept {
        assert_storage_valid();
        const auto start = m_process_head;
        const auto head  = m_queue_head.load_acquire();
        const auto delta = head - start;

        u32 result = 0U;
        if (0U < delta) {
            for (; (result < delta) && (result < f_max_count); ++result) {
                const auto index      = (start + result) & Mask;
                f_out_objects[result] = &m_objects[index];
            }

            m_process_head += result;
        }

        return result;
    }

    //! [SCSP] {Consumer} Collect objects that need processing (up to \p f_max_count)
    //! \remark \p f_out_remaining will contain the count of remaining pending objects to be processed by the consumer
    [[nodiscard]] u32 dequeue_burst_hint(_Object** f_out_objects, u32 f_max_count, u64& f_out_remaining) noexcept {
        const auto start = m_process_head;
        const auto head  = m_queue_head.load_acquire();
        const auto delta = head - start;

        u32 result = 0U;
        if (0U < delta) {
            for (; (result < delta) && (result < f_max_count); ++result) {
                const auto index      = (start + result) & Mask;
                f_out_objects[result] = &m_objects[index];
            }

            m_process_head += result;
        }

        //Save remaining
        f_out_remaining = delta - result;

        return result;
    }

    //! [SCSP] {Producer} Get processed objects (up to \p f_max_count)
    [[nodiscard]] u32 dequeue_results_burst(_Object** f_out_buffer, u32 f_max_count) noexcept {
        const auto start = m_collect_tail;
        const auto end   = m_queue_tail.load_acquire();
        const auto delta = end - start;

        u32 result = 0U;
        if (0U < delta) {
            for (; (result < delta) && (result < f_max_count); ++result) {
                const auto index     = (start + result) & Mask;
                f_out_buffer[result] = &m_objects[index];
            }
            m_collect_tail += result;
        }
        return result;
    }

    //! [SCSP] {Producer} Process results via \p _StaticFunctor
    //! \remark [](_Object& f_object) static noexcept -> void {}
    //! \return the count of processed objects
    template <u32 _MaxProcessCount, typename _StaticFunctor>
    [[nodiscard]] u32 process_results() noexcept {
        const auto start = m_collect_tail;
        const auto end   = m_queue_tail.load_acquire();
        const auto delta = end - start;

        u32 result = 0U;
        if (0U < delta) {
            for (; (result < delta) && (result < _MaxProcessCount); ++result) {
                const auto      index = (start + result) & Mask;
                _StaticFunctor::operator()(m_objects[index]);
            }
            m_collect_tail += result;
        }
        return result;
    }

    //! [SCSP] {Producer} Process results via \p _StaticFunctor
    //! \remark [](_Object& f_object) static noexcept -> void {}
    //! \return the count of processed objects
    template <typename _StaticFunctor>
    [[nodiscard]] u32 process_results(u32 f_max_process_count) noexcept {
        const auto start = m_collect_tail;
        const auto end   = m_queue_tail.load_acquire();
        const auto delta = end - start;

        u32 result = 0U;
        if (0U < delta) {
            for (; (result < delta) && (result < f_max_process_count); ++result) {
                const auto      index = (start + result) & Mask;
                _StaticFunctor::operator()(m_objects[index]);
            }
            m_collect_tail += result;
        }
        return result;
    }

    //! [SCSP] {Producer} Process results via \p _Functor
    //! \remark [](_Object& f_object) noexcept -> void {}
    //! \return the count of processed objects
    template <typename _Functor>
    [[nodiscard]] u32 process_results(u32 f_max_process_count, _Functor& f_functor) noexcept {
        const auto start = m_collect_tail;
        const auto end   = m_queue_tail.load_acquire();
        const auto delta = end - start;

        u32 result = 0U;
        if (0U < delta) {
            for (; (result < delta) && (result < f_max_process_count); ++result) {
                const auto index = (start + result) & Mask;
                f_functor(m_objects[index]);
            }
            m_collect_tail += result;
        }
        return result;
    }

    //! [SCSP] {Producer} Discard all results (if any)
    template <bool _ResetObjectsIfPossible = true>
    u32 discard_results() noexcept {
        const auto start = m_collect_tail;
        const auto end   = m_queue_tail.load_acquire();
        const auto delta = end - start;

        if (0U < delta) {
            //Immediately collect all results
            m_collect_tail = end;

            //Reset all objects if possible and free them
            if constexpr (_ResetObjectsIfPossible && spsc_bidirectional_ring_reset_method_c<_Object>) {
                submit_processed_results();
            } else {
                m_end_tail = end;
            }
        }

        return delta;
    }

    //! [SCSP] {Producer} Get processed objects (up to \p f_max_count)
    //! \remark \p f_out_remaining will contain the count of remaining pending objects to be processed by the producer (results)
    [[nodiscard]] u32 dequeue_results_burst_hint(_Object** f_out_buffer, u32 f_max_count, u64& f_out_remaining) noexcept {
        const auto start = m_collect_tail;
        const auto end   = m_queue_tail.load_acquire();
        const auto delta = end - start;

        u32 result = 0U;
        if (0U < delta) {
            for (; (result < delta) && (result < f_max_count); ++result) {
                const auto index     = (start + result) & Mask;
                f_out_buffer[result] = &m_objects[index];
            }
            m_collect_tail += result;
        }

        //Save remaining
        f_out_remaining = delta - result;

        return result;
    }

    //! [SCSP] {Producer} Submit all processed (results) objects (free objects back to the queue)
    //! \remark if _Object has a method reset() noexcept, it will be called otherwise objects are left as they are
    void submit_processed_results() noexcept {
        if constexpr (spsc_bidirectional_ring_reset_method_c<_Object>) {
            while (m_end_tail != m_collect_tail) {
                m_objects[m_end_tail & Mask].reset();
                ++m_end_tail;
            }
        } else {
            m_end_tail = m_collect_tail;
        }
    }

    //! [SCSP] {Producer} Submit \p f_count processed (results) objects (free objects back to the queue)
    //! \remark if _Object has a method reset() noexcept, it will be called otherwise objects are left as they are
    void submit_processed_results(u32 f_count) noexcept {
        SKL_ASSERT(f_count <= (m_collect_tail - m_end_tail));
        if constexpr (spsc_bidirectional_ring_reset_method_c<_Object>) {
            for (u32 i = 0; i < f_count; ++i, ++m_end_tail) {
                m_objects[m_end_tail & Mask].reset();
            }
        } else {
            m_end_tail += f_count;
        }
    }

private:
    //! Assert that storage is valid (only relevant for hugepage mode)
    void assert_storage_valid() const noexcept {
        if constexpr (_UseHugePages) {
            SKL_ASSERT_PERMANENT(nullptr != m_objects && "Internal storage not allocated: call allocate_internal_storage() first");
        }
    }

    SKL_CACHE_ALIGNED storage_t m_objects{}; //!< {Producer & Consumer} All queue objects

    SKL_CACHE_ALIGNED u64 m_allocate_head = 0ULL; //!< {Producer} Head to allocate at
    u64                   m_collect_tail  = 0ULL; //!< {Producer} Current collect tail
    u64                   m_end_tail      = 0ULL; //!< {Producer} Current queue end tail

    SKL_CACHE_ALIGNED std::relaxed_value<u64> m_queue_head = 0ULL; //!< {Producer -> Consumer} Current enqueue head
    SKL_CACHE_ALIGNED std::relaxed_value<u64> m_queue_tail = 0ULL; //!< {Consumer -> Producer} Current enqueue tail

    SKL_CACHE_ALIGNED u64 m_process_head = 0ULL; //!< {Consumer} Current process head
};
} // namespace skl
