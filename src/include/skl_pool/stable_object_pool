//!
//! \file stable_object_pool
//!
//! \brief A stable object pool implementation
//!
#pragma once

#include "skl_def"
#include "skl_pair"
#include "skl_fixed_vector_if"
#include "skl_vector_if"
#include "skl_dynamic_bitset"
#include "skl_huge_pages"

namespace skl {
namespace stable_object_pool {
    template <typename _Block, bool _UseCoreAlloc, bool _UseHugePages>
    struct block_allocator_t;

    template <typename _Object, u64 _BlockSize, bool _ConstructAndDestruct = true, bool _UseCoreAlloc = false, typename _IndexType = u16>
    struct object_pool_t;
} // namespace stable_object_pool

template <typename _Object,
          u64  _BlockSize,
          bool _ConstructAndDestruct = true,
          bool _UseCoreAlloc         = false,
          bool _UseHugePages         = false>
class StableObjectPool;

namespace stable_object_pool {
    template <typename _Object, u64 _BlockSize, bool _ConstructAndDestruct, bool _UseCoreAlloc, typename _IndexType>
    struct object_pool_t {
        struct alignas(alignof(_Object)) object_placeholder_t {
            byte body[sizeof(_Object)]{};
        };

        using index_t      = _IndexType;
        using block_t      = skl::skl_fixed_vector_impl<object_placeholder_t, _BlockSize, SKL_CACHE_LINE_SIZE, false, _UseCoreAlloc>;
        using free_stack_t = skl::skl_fixed_vector_impl<index_t, _BlockSize, alignof(index_t), false, _UseCoreAlloc>;

        static_assert(_BlockSize > 0U, "_BlockSize must be grater then 0");
        static_assert(_BlockSize <= static_cast<u64>(skl_numeric_limits<index_t>::max()), "_BlockSize exceeds index type capacity");

        object_pool_t() noexcept {
            //Fill the free stack
            index_t free_indices = 0u;
            while (free_indices < static_cast<index_t>(_BlockSize)) {
                m_free.upgrade().push_back(free_indices++);
            }

            m_block_start = reinterpret_cast<_Object*>(m_block.data());
            m_block_end   = m_block_start + _BlockSize;

            m_block.upgrade().grow(m_block.capacity());
        }

        ~object_pool_t() noexcept {
            clear();
        }

        SKL_NO_MOVE_OR_COPY(object_pool_t);

        //! Allocate a new object, returns nullptr if no free object
        template <typename... _Args>
        [[nodiscard]] _Object* allocate(_Args... f_args) noexcept(__is_nothrow_constructible(_Object, _Args...))
            requires(_ConstructAndDestruct)
        {
            if (m_free.empty()) {
                return nullptr;
            }

            //Get the index of the free object
            const auto index = m_free.back();
            m_free.upgrade().pop_back();

            //Construct the object in place
            auto* place = reinterpret_cast<_Object*>(&m_block[index]);
            new (place) _Object(skl_fwd<_Args>(f_args)...);

            return place;
        }

        //! Allocate a new raw object
        //! \returns nullptr if no free object
        //! \remark Object is not constructed
        [[nodiscard]] _Object* allocate_raw() noexcept {
            if (m_free.empty()) {
                return nullptr;
            }

            //Get the index of the free object
            const auto index = m_free.back();
            m_free.upgrade().pop_back();

            //Do not construct the object
            auto* place = reinterpret_cast<_Object*>(&m_block[index]);

            return place;
        }

        //! Allocate a new raw object
        //! \remark Object is not constructed
        //! \remark Asserts there is a free object
        [[nodiscard]] _Object* allocate_raw_checked() noexcept {
            SKL_ASSERT_CRITICAL(false == m_free.empty());

            //Get the index of the free object
            const auto index = m_free.back();
            m_free.upgrade().pop_back();

            //Do not construct the object
            auto* place = reinterpret_cast<_Object*>(&m_block[index]);

            return place;
        }

        //! Deallocate the given object
        //! \returns false if the object is out of range
        [[nodiscard]] bool deallocate_safe(_Object* f_object) noexcept((false == _ConstructAndDestruct) || __is_nothrow_destructible(_Object)) {
            if (false == owns(f_object)) {
                return false;
            }

            if constexpr (_ConstructAndDestruct) {
                //Call the destructor
                f_object->~_Object();
            }

            //Calculate the index of the object
            const auto index = static_cast<index_t>(f_object - m_block_start);

#if !SKL_BUILD_SHIPPING
            //Make sure the index is valid
            SKL_ASSERT_CRITICAL(index < _BlockSize);

            //Make sure the object is not already deallocated
            SKL_ASSERT_CRITICAL(false == m_free.upgrade().contains(index));
#endif

            m_free.upgrade().push_back(index);

            return true;
        }

        //! Deallocate the given object
        //! \remark Asserts the object is in range
        void deallocate(_Object* f_object) noexcept((false == _ConstructAndDestruct) || __is_nothrow_destructible(_Object)) {
            SKL_ASSERT_CRITICAL(owns(f_object));

            if constexpr (_ConstructAndDestruct) {
                //Call the destructor
                f_object->~_Object();
            }

            //Calculate the index of the object
            const auto index = static_cast<index_t>(f_object - m_block_start);

#if !SKL_BUILD_SHIPPING
            //Make sure the index is valid
            SKL_ASSERT_CRITICAL(index < _BlockSize);

            //Make sure the object is not already deallocated
            SKL_ASSERT_CRITICAL(false == m_free.upgrade().contains(index));
#endif

            m_free.upgrade().push_back(index);
        }

        //! Get the number of free objects
        [[nodiscard]] u64 free_count() const noexcept {
            return m_free.size();
        }

        //! Get the number of used objects
        [[nodiscard]] u64 size() const noexcept {
            return capacity() - free_count();
        }

        //! Get the number of objects in the pool
        [[nodiscard]] constexpr u64 capacity() const noexcept {
            return _BlockSize;
        }

        //! Is the pool full
        [[nodiscard]] constexpr bool full() const noexcept {
            return m_free.empty();
        }

        //! Is the pool empty
        [[nodiscard]] constexpr bool empty() const noexcept {
            return m_free.size() == _BlockSize;
        }

        //! Is the object from this pool
        [[nodiscard]] constexpr bool owns(const _Object* f_object) const noexcept {
            return (f_object >= m_block_start) && (f_object < m_block_end);
        }

        //! Clear the pool, destroying all objects if needed
        //! \remark Î¸(n * m) where n is the block size and m is the number of used objects
        constexpr void clear() noexcept((false == _ConstructAndDestruct) || __is_nothrow_destructible(_Object)) {
            if constexpr (_ConstructAndDestruct) {
                //Destroy all used objects
                for (index_t i = 0u; i < static_cast<index_t>(_BlockSize); ++i) {
                    //If the object is not in the free stack, it is used
                    if (false == m_free.upgrade().contains(i)) {
                        auto* place = reinterpret_cast<_Object*>(&m_block[i]);
                        place->~_Object();
                    }
                }
            }

            m_free.clear();

            //Refill the free stack
            index_t free_indices = 0u;
            while (free_indices < static_cast<index_t>(_BlockSize)) {
                m_free.upgrade().push_back(free_indices++);
            }
        }

    private:
        friend class StableObjectPool<_Object, _BlockSize, _ConstructAndDestruct, _UseCoreAlloc, false>;
        friend class StableObjectPool<_Object, _BlockSize, _ConstructAndDestruct, _UseCoreAlloc, true>;

        block_t      m_block;       //!< Storage for objects
        free_stack_t m_free;        //!< Stack of free object indices
        _Object*     m_block_start; //!< Start of the object block
        _Object*     m_block_end;   //!< End of the object block (last valid object in the block)
    };

    template <typename _Block, bool _UseCoreAlloc, bool _UseHugePages>
    struct block_allocator_t {
        //! [Allocator] Allocate a new block
        //!
        //! \returns Pointer to allocated block
        //!
        //! \remark When _UseHugePages: allocates exactly 1 huge page (2MB)
        //! \remark sizeof(_Block) must fit in 2MB (enforced by static_assert)
        [[nodiscard]] static _Block* alloc() noexcept(__is_nothrow_constructible(_Block)) {
            void* memory_block = nullptr;

            if constexpr (_UseHugePages) {
                SKL_ASSERT_PERMANENT(huge_pages::is_huge_pages_enabled());
                static_assert(sizeof(_Block) <= huge_pages::CHugePageSize,
                              "Block size must fit in one 2MB huge page");

                // Allocate exactly 1 huge page (2MB)
                memory_block = huge_pages::skl_huge_page_alloc(1u);
            } else if constexpr (_UseCoreAlloc) {
                memory_block = skl_core_alloc(sizeof(_Block), alignof(_Block));
            } else {
                memory_block = skl_vector_alloc(sizeof(_Block), alignof(_Block));
            }

            SKL_ASSERT_PERMANENT(nullptr != memory_block);
            new (memory_block) _Block();
            return reinterpret_cast<_Block*>(memory_block);
        }

        //! [Allocator] Free the given block
        //!
        //! \param f_block Block to free
        //!
        //! \remark When _UseHugePages: frees exactly 1 huge page (2MB)
        static void free(_Block* f_block) noexcept {
            if (nullptr == f_block) {
                return;
            }

            f_block->~_Block();

            if constexpr (_UseHugePages) {
                // Free exactly 1 huge page (2MB)
                huge_pages::skl_huge_page_free(f_block, 1u);
            } else if constexpr (_UseCoreAlloc) {
                skl_core_free(f_block);
            } else {
                skl_vector_free(f_block);
            }
        }
    };
} // namespace stable_object_pool

template <typename _Object, u64 _BlockSize, bool _ConstructAndDestruct, bool _UseCoreAlloc, bool _UseHugePages>
class StableObjectPool {
public:
    static constexpr u64 CObjectSize = sizeof(_Object);

    // Use u32 indices for huge pages (allows up to 4 billion objects), u16 otherwise
    using index_t = conditional_t<_UseHugePages, u32, u16>;

    //! [Compiletime] Calculate optimal block size for huge pages
    //!
    //! \returns Number of objects that fit in one 2MB page with pool_t overhead
    //!
    //! \remark Solves: sizeof(pool_t<_Object, N, ...>) <= 2MB for maximum N
    //! \remark pool_t overhead per object: sizeof(_Object) + sizeof(index_t)
    //! \remark Fixed overhead: 6 pointers + alignment padding
    [[nodiscard]] static constexpr u64 calculate_huge_page_block_size() noexcept {
        constexpr u64 huge_page_size = huge_pages::CHugePageSize;
        constexpr u64 pointer_overhead = (6ull * sizeof(void*)); // 6 pointers in pool_t
        constexpr u64 per_object_cost = sizeof(_Object) + sizeof(index_t); // object + free index

        // Conservative padding for alignment
        constexpr u64 alignment_padding = 256ull;

        // Solve for N: huge_page_size = pointer_overhead + (N * per_object_cost) + padding
        constexpr u64 available_space = huge_page_size - pointer_overhead - alignment_padding;
        constexpr u64 calculated_size = available_space / per_object_cost;

        // Clamp to index_t max
        constexpr u64 max_by_index = static_cast<u64>(skl_numeric_limits<index_t>::max());
        constexpr u64 max_objects = (calculated_size <= max_by_index) ? calculated_size : max_by_index;

        // Must have at least 1 object
        static_assert(max_objects > 0u, "Object too large to fit in huge page with overhead");

        return max_objects;
    }

    static constexpr u64 CHugePagesBlockSize = _UseHugePages ? calculate_huge_page_block_size() : 0u;
    static constexpr u64 CBlockSize          = _UseHugePages ? CHugePagesBlockSize : _BlockSize;

    using pool_t            = stable_object_pool::object_pool_t<_Object, CBlockSize, _ConstructAndDestruct, _UseCoreAlloc, index_t>;
    using free_bitset_t     = skl::DynamicBitSet<false>; // Bit=0: has free space, Bit=1: full
    using block_allocator_t = stable_object_pool::block_allocator_t<pool_t, _UseCoreAlloc, _UseHugePages>;

    // Invariant: When using huge pages, pool_t must fit in 2MB
    static_assert(!_UseHugePages || (sizeof(pool_t) <= huge_pages::CHugePageSize),
                  "pool_t must fit within one 2MB huge page");

    StableObjectPool() noexcept = default;
    ~StableObjectPool() noexcept {
        clear();
    }

    SKL_NO_MOVE_OR_COPY(StableObjectPool);

    //! Allocate a new object
    //! \returns nullptr if no free object
    template <typename... _Args>
        requires(_ConstructAndDestruct)
    [[nodiscard]] _Object* allocate(_Args... f_args) noexcept(__is_nothrow_constructible(_Object, _Args...)) {
        auto* obj = allocate_raw();
        if (nullptr == obj) {
            return nullptr;
        }

        //Construct the object in place
        new (obj) _Object(skl_fwd<_Args>(f_args)...);

        [[likely]] return obj;
    }

    //! Allocate a new raw object
    //! \returns nullptr if no free object
    //! \remark Object is not constructed
    [[nodiscard]] _Object* allocate_raw() noexcept {
        if (m_pools.empty()) [[unlikely]] {
            goto new_pool;
        }

        // Most likely: current pool has space
        if (false == m_pools[m_current_pool_index]->full()) [[likely]] {
            goto allocate;
        }

        // Current pool is full, mark it and search for another
        m_free_pools.set(m_current_pool_index);

        // Search for a non-full pool using SIMD-optimized find (bit=0 means has free space)
        {
            auto result = m_free_pools.find_first<false>();
            if (result.is_success()) {
                m_current_pool_index = result.value();
                goto allocate;
            }
        }

        // All pools are full, create a new one
        goto new_pool;

    allocate:
        {
            SKL_ASSERT(false == m_pools[m_current_pool_index]->full());
            auto* obj = m_pools[m_current_pool_index]->allocate_raw_checked();

            // If pool became full after this allocation, mark it
            if (m_pools[m_current_pool_index]->full()) {
                m_free_pools.set(m_current_pool_index);
            }

            return obj;
        }
    new_pool:
        {
            auto* new_block = block_allocator_t::alloc();
            m_pools.upgrade().push_back(new_block);
            m_current_pool_index = u32(m_pools.size() - 1u);
            m_pool_ranges.upgrade().emplace_back(skl::pair<_Object*, _Object*>(new_block->m_block_start, new_block->m_block_end));
            m_free_pools.grow(u32(m_pools.size())); // Grow bitset, new bit defaults to 0 (has free space)
            SKL_ASSERT_PERMANENT((m_pools.size() == m_pool_ranges.size()) && (m_pools.size() == m_free_pools.size()));
            goto allocate;
        }
    }

    //! Deallocate the given object
    //! \returns false if the object is out of range
    //! \remark Asserts that the object is not already deallocated
    [[nodiscard]] bool deallocate_safe(_Object* f_object) noexcept {
        SKL_ASSERT_PERMANENT((m_pools.size() == m_pool_ranges.size()) && (m_free_pools.size() == m_pool_ranges.size()));
        //Find the pool that owns this object
        for (u64 i = 0u; i < m_pool_ranges.size(); ++i) {
            const auto& range = m_pool_ranges[i];
            if ((f_object >= range.first) && (f_object < range.second)) {
                const bool was_full = m_pools[i]->full();
                m_pools[i]->deallocate(f_object);

                // If pool was full and now has free space, unset the bit
                if (was_full) {
                    m_free_pools.unset(u32(i));
                }

                m_current_pool_index = u32(i);
                return true;
            }
        }

        return false;
    }

    //! Deallocate the given object
    //! \remark Asserts the object is in range
    //! \remark Asserts that the object is not already deallocated
    void deallocate(_Object* f_object) noexcept {
        //Find the pool that owns this object
        for (u64 i = 0u; i < m_pool_ranges.size(); ++i) {
            const auto [range_start, range_end] = m_pool_ranges[i];
            if ((f_object >= range_start) && (f_object < range_end)) {
                const bool was_full  = m_pools[i]->full();
                m_current_pool_index = u32(i);
                m_pools[i]->deallocate(f_object);

                // If pool was full and now has free space, unset the bit
                if (was_full) {
                    m_free_pools.unset(u32(i));
                }

                f_object = nullptr;
                break;
            }
        }

        // Assert object was found (nullptr check) + avoids unused parameter warning when asserts disabled
        SKL_ASSERT_PERMANENT(nullptr == f_object);
    }

    //! Is the object from this pool
    [[nodiscard]] bool owns(const _Object* f_object) const noexcept {
        //Find the pool that owns this object
        for (auto [range_start, range_end] : m_pool_ranges) {
            if ((f_object >= range_start) && (f_object < range_end)) {
                return true;
            }
        }

        return false;
    }

    //! Get total no of objects this pool can allocate
    [[nodiscard]] u64 capacity() const noexcept {
        return CBlockSize * m_pools.size();
    }

    //! Get total no of objects allocated
    [[nodiscard]] u64 size() const noexcept {
        u64 total = 0u;
        for (const auto* pool : m_pools) {
            total += pool->size();
        }
        return total;
    }

    //! Get total no of bytes allocated from the backing storage
    [[nodiscard]] u64 mem_usage() const noexcept {
        return u64(sizeof(pool_t) * m_pools.size())
             + u64(sizeof(void*) * m_pools.capacity())
             + u64(sizeof(free_bitset_t))
             + u64(sizeof(free_bitset_t::slice_t) * m_free_pools.slices_count())
             + u64(sizeof(skl::pair<_Object*, _Object*>) * m_pool_ranges.capacity());
    }

    //! Get total no of bytes allocated from the backing storage for objects only
    [[nodiscard]] u64 mem_usage_objects() const noexcept {
        return (sizeof(_Object) * CBlockSize) * m_pools.size();
    }

    //! Is the pool empty
    [[nodiscard]] bool empty() const noexcept {
        return size() == 0u;
    }

    //! Clear the pool, destroying all objects if needed
    void clear() noexcept {
        m_current_pool_index = 0u;
        for (auto* pool : m_pools) {
            block_allocator_t::free(pool);
        }
        m_pools.clear();
        m_pool_ranges.clear();
        m_free_pools.clear();
    }

private:
    u32                                            m_current_pool_index{0u}; //!< Index of the next pool to allocate from
    skl::skl_vector<pool_t*, 8u>                   m_pools{};                //!< Vector of pools
    free_bitset_t                                  m_free_pools;             //!< Bitset of free pools
    skl::skl_vector<skl::pair<_Object*, _Object*>> m_pool_ranges{};          //!< Vector of pool ranges for quick lookup
};
} // namespace skl
